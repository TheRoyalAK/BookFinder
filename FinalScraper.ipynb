{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Useful Imports__\n",
    "\n",
    "1. __Pandas:__ This library was used to handle the data.\n",
    "\n",
    "2. __Warnings:__ This was only to ignore the warnings.\n",
    "\n",
    "3. __Requests:__ This was used to send requests to APIs and websites to get the data.\n",
    "\n",
    "4. __BeautifulSoup:__ This library was used to extract data from the source code of websites.\n",
    "\n",
    "5. __Pyisbn:__ This was used later to convert ISBN10 to ISBN13 to search for books on bookswagon.com.\n",
    "\n",
    "6. __Concurrent:__ This was used to do multithreaded scraping.\n",
    "\n",
    "7. __Time:__ This was used to insert a delay if the threads are sending requests too quick and getting denied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from requests import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pyisbn\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Ingestion__\n",
    "\n",
    "The following data was collected from the RC of the college with the following fields:-\n",
    "\n",
    "1. __Acc. Date__:- This is the date in which the RC acquired the book.\n",
    "\n",
    "2. __Acc. No.__:- This is the number of book that was assigned to it when acquiring it.\n",
    "\n",
    "3. __Title__:- This is the title of the book.\n",
    "\n",
    "4. __ISBN__:- This is the ISBN of the book. (We will mainly focus on this)\n",
    "\n",
    "5. __Ed./Vol.__:- This signifies if the book was a different edition (than first edition).\n",
    "\n",
    "6. __Place & Publisher__:- This tells us about the place and name of the publisher.\n",
    "\n",
    "7. __Year__:- This is the year of publishing the book.\n",
    "\n",
    "8. __Pages__:- This constitutes of the amount of pages the book has.\n",
    "\n",
    "9. __Class No./Book No.__:- This gives us another identifier for the book.\n",
    "\n",
    "The data was cleaned manually as well as programmatically. It had a lot of inconsistency due to titles of the books containing `,` and `;` both of which are used in `.csv` as delimiters. Some of the ISBNs were wrong and dates were wrong too. ~400 books were removed due to having no ISBN or wrong ISBNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc. Date</th>\n",
       "      <th>Acc. No.</th>\n",
       "      <th>Title</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Author/Editor</th>\n",
       "      <th>Ed./Vol.</th>\n",
       "      <th>Place &amp; Publisher</th>\n",
       "      <th>Year</th>\n",
       "      <th>Page(s)</th>\n",
       "      <th>Class No./Book No.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>20-01-2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Network design : management and technical pers...</td>\n",
       "      <td>849334047</td>\n",
       "      <td>Mann-Rubinson, Teresa C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boca Raton: CRC Press,</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>405 p.</td>\n",
       "      <td>004.6 MAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30290</th>\n",
       "      <td>08-09-2001</td>\n",
       "      <td>2</td>\n",
       "      <td>Multimedia information analysis and retrieval ...</td>\n",
       "      <td>9783540648260</td>\n",
       "      <td>Ip, Horace H. S.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Berlin: Springer,</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>viii, 264 p.;</td>\n",
       "      <td>004 IPH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>08-09-2001</td>\n",
       "      <td>3</td>\n",
       "      <td>Multimedia systems : delivering, generating, a...</td>\n",
       "      <td>1852332484</td>\n",
       "      <td>Morris, Tim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London: Springer,</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>xi, 191 p.;</td>\n",
       "      <td>006.7 MOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30239</th>\n",
       "      <td>05-09-2001</td>\n",
       "      <td>4</td>\n",
       "      <td>Principles of Data Mining and Knowledge Discovery</td>\n",
       "      <td>9783540410669</td>\n",
       "      <td>Zytkov, Jan. M.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York: Springer-Verlag,</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>593 p.</td>\n",
       "      <td>006.3 ZYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6727</th>\n",
       "      <td>09-08-2001</td>\n",
       "      <td>5</td>\n",
       "      <td>Focusing solutions for data mining : analytica...</td>\n",
       "      <td>3540664297</td>\n",
       "      <td>Reinartz, Thomas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York: Springer,</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>xiv, 307 p.;</td>\n",
       "      <td>006.3 REI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15078</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>36354</td>\n",
       "      <td>The Ruba'iyat of Omar Khayyam</td>\n",
       "      <td>9780140443844</td>\n",
       "      <td>Khayyam, Omar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London : Penguin Books,</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>116 p.</td>\n",
       "      <td>891.5511 KHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28586</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>36355</td>\n",
       "      <td>Artificial intelligence for robotics</td>\n",
       "      <td>9781805129592</td>\n",
       "      <td>Francis X.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UK : Packt Publishing,</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>xvii, 325 p. ;</td>\n",
       "      <td>006.3 FRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15404</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>36356</td>\n",
       "      <td>Too big to fail: The inside story of how wall ...</td>\n",
       "      <td>9780143118244</td>\n",
       "      <td>Sorkin, Andrew Ross</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York : Penguin Books,</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>xx, 618p. ;</td>\n",
       "      <td>330.973 SOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25384</th>\n",
       "      <td>07-01-2026</td>\n",
       "      <td>36357</td>\n",
       "      <td>Digital communications : A foundational approach</td>\n",
       "      <td>9781009429665</td>\n",
       "      <td>Fischer, Robert F. H.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambridge : Cambridge University Press,</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>xiv, 397p. ;</td>\n",
       "      <td>621.382 FIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25500</th>\n",
       "      <td>07-01-2026</td>\n",
       "      <td>36358</td>\n",
       "      <td>Grey sister</td>\n",
       "      <td>9781101988909</td>\n",
       "      <td>Lawrence, Mark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Berkley : Penguin,</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>xiv, 397p. ;</td>\n",
       "      <td>813.6 LAW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31540 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Acc. Date  Acc. No.  \\\n",
       "4705   20-01-2017         1   \n",
       "30290  08-09-2001         2   \n",
       "5926   08-09-2001         3   \n",
       "30239  05-09-2001         4   \n",
       "6727   09-08-2001         5   \n",
       "...           ...       ...   \n",
       "15078  05-01-2026     36354   \n",
       "28586  05-01-2026     36355   \n",
       "15404  05-01-2026     36356   \n",
       "25384  07-01-2026     36357   \n",
       "25500  07-01-2026     36358   \n",
       "\n",
       "                                                   Title           ISBN  \\\n",
       "4705   Network design : management and technical pers...      849334047   \n",
       "30290  Multimedia information analysis and retrieval ...  9783540648260   \n",
       "5926   Multimedia systems : delivering, generating, a...     1852332484   \n",
       "30239  Principles of Data Mining and Knowledge Discovery  9783540410669   \n",
       "6727   Focusing solutions for data mining : analytica...     3540664297   \n",
       "...                                                  ...            ...   \n",
       "15078                      The Ruba'iyat of Omar Khayyam  9780140443844   \n",
       "28586               Artificial intelligence for robotics  9781805129592   \n",
       "15404  Too big to fail: The inside story of how wall ...  9780143118244   \n",
       "25384   Digital communications : A foundational approach  9781009429665   \n",
       "25500                                        Grey sister  9781101988909   \n",
       "\n",
       "                  Author/Editor Ed./Vol.  \\\n",
       "4705   Mann-Rubinson, Teresa C.      NaN   \n",
       "30290          Ip, Horace H. S.      NaN   \n",
       "5926                Morris, Tim      NaN   \n",
       "30239           Zytkov, Jan. M.      NaN   \n",
       "6727           Reinartz, Thomas      NaN   \n",
       "...                         ...      ...   \n",
       "15078             Khayyam, Omar      NaN   \n",
       "28586                Francis X.      NaN   \n",
       "15404       Sorkin, Andrew Ross      NaN   \n",
       "25384     Fischer, Robert F. H.      NaN   \n",
       "25500            Lawrence, Mark      NaN   \n",
       "\n",
       "                             Place & Publisher    Year         Page(s)  \\\n",
       "4705                    Boca Raton: CRC Press,  1999.0          405 p.   \n",
       "30290                        Berlin: Springer,  1998.0   viii, 264 p.;   \n",
       "5926                         London: Springer,  2000.0     xi, 191 p.;   \n",
       "30239               New York: Springer-Verlag,  1999.0          593 p.   \n",
       "6727                       New York: Springer,  1999.0    xiv, 307 p.;   \n",
       "...                                        ...     ...             ...   \n",
       "15078                  London : Penguin Books,  1981.0         116 p.    \n",
       "28586                   UK : Packt Publishing,  2024.0  xvii, 325 p. ;   \n",
       "15404                New York : Penguin Books,  2010.0     xx, 618p. ;   \n",
       "25384  Cambridge : Cambridge University Press,  2024.0    xiv, 397p. ;   \n",
       "25500                       Berkley : Penguin,  2019.0    xiv, 397p. ;   \n",
       "\n",
       "      Class No./Book No.  \n",
       "4705           004.6 MAN  \n",
       "30290            004 IPH  \n",
       "5926           006.7 MOR  \n",
       "30239          006.3 ZYT  \n",
       "6727           006.3 REI  \n",
       "...                  ...  \n",
       "15078       891.5511 KHA  \n",
       "28586          006.3 FRA  \n",
       "15404        330.973 SOR  \n",
       "25384        621.382 FIS  \n",
       "25500          813.6 LAW  \n",
       "\n",
       "[31540 rows x 10 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = './Accession Register-Books _with_ ISBN_numbers.xlsx'\n",
    "data_table = pd.read_excel(data_file)\n",
    "data_table.drop_duplicates(subset='ISBN', keep='first', inplace=True)\n",
    "data_table.sort_values('Acc. No.', inplace=True)\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Scraping__\n",
    "\n",
    "This is the header that was used which was taken after visiting a website and going into the developer mode in firefox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first made a scraper that would scrap books from the Google Books site and then if it doesn't find summary there it would fallback to goodreads.com.\n",
    "\n",
    "Later, we realised that google provides an API for books which would be faster than scraping their sites and hence we switched our method. We also realised that alot of the summaries provided by goodreads.com were not of good quality and hence in the second method we stopped using that and searched for better alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING GOOGLE WEBSITE\n",
    "\n",
    "def summary_finder(isbn, header):\n",
    "    try:\n",
    "        summary = \"\"\n",
    "        isbn = str(isbn)\n",
    "        if len(isbn) < 10:\n",
    "            isbn = f\"{'0'*(10 - len(isbn))}{isbn}\"\n",
    "        print(isbn)\n",
    "        search_base_link = f'https://books.google.com/books?vid=ISBN{isbn}'\n",
    "        search_req = request(method='GET', url=search_base_link, headers=header)\n",
    "        book_soup = BeautifulSoup(search_req.text, 'html.parser')\n",
    "        # print('initial google search')\n",
    "        if book_soup.find(name='a', attrs={'class': 'opt-in-header-link'}):\n",
    "            if book_soup.find(name='a', attrs={'class': 'opt-in-header-link'}).text  == \"Try the new Google Books\":\n",
    "                new_link = book_soup.find(name='a', attrs={'class': 'opt-in-header-link'}).attrs['href']\n",
    "                new_req = request(method='GET', url=new_link, headers=header)\n",
    "                new_soup = BeautifulSoup(new_req.text, 'html.parser')\n",
    "                # print('new google search')\n",
    "                book_soup = new_soup\n",
    "        # print('finding summary in google')\n",
    "        summary = book_soup.find(name='div', attrs={'class': 'Mhmsgc'}) or book_soup.find(name='div', attrs={'id': 'synopsistext'})\n",
    "        if not summary:\n",
    "            # print('goodreads')\n",
    "            search_base_link = f'https://www.goodreads.com/search?utf8=%E2%9C%93&search%5Bquery%5D={isbn}'\n",
    "            search_req = request(method='GET', url=search_base_link, headers=header)\n",
    "            book_soup = BeautifulSoup(search_req.text, 'html.parser')\n",
    "            summary = book_soup.find(name='span', attrs={'class': 'Formatted'})\n",
    "    except:\n",
    "        raise ReferenceError(f\"{isbn} failed\")\n",
    "        # return isbn, \"FAILED\"\n",
    "    if summary:\n",
    "        for br in summary.find_all('br'):\n",
    "            br.replace_with('\\\\n')\n",
    "        return isbn, summary.text\n",
    "    return isbn, \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that openLibrary has its own API as well and started using it. We noticed that google and openlibrary both give us category/subject which can be really helpful for the semantic search that the data science team would perform and hence came to the conclusion of saving the keywords as well. Also, we found out about the site bookswagon.com which had summary and keywords for a lot more of the books that was not available from google as well as openlibrary.\n",
    "\n",
    "Our function first tries google, then goes for OpenLibrary and then finally bookswagon.com as a fallback. If at any time it finds keywords as well as summary, it directly returns the isbn, keywords and summary as a tuple. And when going from one site to another, it only keeps the higher quality summary/keywords and does not replace it. If the ISBN is given wrong, the code would throw an error at line 60, which tries to convert the ISBN, and so we added a try-except catch to write the ISBNs in a new file that shows the wrong ISBNs.\n",
    "\n",
    "<div style=\"background-color: #e7f3ff; padding: 15px; border-radius: 5px; border-left: 5px solid #007acc; color: #444; width: 95%\">\n",
    "    <b>Note:</b> \n",
    "    <br>\n",
    "    We were not able to scrap much using this function, the details are given later in this file.\n",
    "    <br>\n",
    "    Also note that we did use an API Key from Google but we removed it before pushing it to GitHub.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google API, OpenLibrary API and bookwagon.com\n",
    "\n",
    "def summary_finder(isbn, header):\n",
    "    try:\n",
    "        summary = \"\"\n",
    "        keywords = []        \n",
    "        isbn = str(isbn)\n",
    "        if len(isbn) < 10:\n",
    "            isbn = f\"{'0'*(10 - len(isbn))}{isbn}\"\n",
    "        print(isbn)\n",
    "        # print('finding summary in google')\n",
    "        api_link = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}\"\n",
    "        api_req = request(method='GET', url=api_link, headers=header)\n",
    "        api_res = api_req.json()\n",
    "        if api_res['totalItems'] != 0:\n",
    "            if 'description' in api_res['items'][0]['volumeInfo']:\n",
    "                summary = api_res['items'][0]['volumeInfo']['description']\n",
    "            if 'categories' in api_res['items'][0]['volumeInfo']:\n",
    "                keywords = api_res['items'][0]['volumeInfo']['categories']\n",
    "        \n",
    "        if keywords and summary:\n",
    "            return isbn, ', '.join(keywords), summary\n",
    "        \n",
    "        print('openlibrary')\n",
    "        api_link = f\"https://www.openlibrary.org/isbn/{isbn}\"\n",
    "        api_req = request(method=\"GET\", url=api_link, headers=header)\n",
    "        try:\n",
    "            api_res = api_req.json()\n",
    "            if 'subjects' in api_res:\n",
    "                if len(keywords) < len(api_res['subjects']):\n",
    "                    keywords = api_res['subjects']\n",
    "            if 'description' in api_res:\n",
    "                if type(api_res['description']) == dict and len(api_res['description']['value']) > len(summary):\n",
    "                    summary = api_res['description']['value']\n",
    "                else:\n",
    "                    if len(api_res['description']) > len(summary):\n",
    "                        summary = api_res['description']\n",
    "            if 'works' in api_res:\n",
    "                api_link = f\"https://www.openlibrary.org{api_res['works'][0]['key']}.json\"\n",
    "                api_req = request(method=\"GET\", url = api_link, headers=header)\n",
    "                api_res = api_req.json()\n",
    "                if 'subjects' in api_res:\n",
    "                    if len(keywords) < len(api_res['subjects']):\n",
    "                        keywords = api_res['subjects']\n",
    "                if 'description' in api_res:\n",
    "                    if type(api_res['description']) == dict and len(api_res['description']['value']) > len(summary):\n",
    "                        summary = api_res['description']['value']\n",
    "                    else:\n",
    "                        if len(api_res['description']) > len(summary):\n",
    "                            summary = api_res['description']\n",
    "            if summary and keywords:\n",
    "                return isbn, ', '.join(keywords), summary\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "            \n",
    "        \n",
    "        print('bookswagon')\n",
    "        if len(isbn) == 10:\n",
    "            isbn = pyisbn.convert(isbn)\n",
    "        search_base_link = f'https://www.bookswagon.com/book/c/{isbn}'\n",
    "        search_req = request(method='GET', url=search_base_link, headers=header)\n",
    "        book_soup = BeautifulSoup(search_req.text, 'html.parser')\n",
    "        summary = book_soup.find(name='div', attrs={'id': 'aboutbook'})\n",
    "        if summary:\n",
    "            summary = summary.p\n",
    "        cats = book_soup.find('ul', attrs={'class': 'blacklistreview'})\n",
    "        if cats:\n",
    "            cats = cats.find_all('a')\n",
    "            cats = list({k.text for k in cats})\n",
    "            if len(cats) > len(keywords):\n",
    "                keywords = cats\n",
    "    except:\n",
    "        with open('./notFound.txt', 'a') as nf:\n",
    "            nf.write(f'{isbn}\\n')\n",
    "        return isbn, \"\", \"\"\n",
    "    if type(summary) != str:\n",
    "        for br in summary.find_all('br'):\n",
    "            br.replace_with('\\\\n')\n",
    "        return isbn, ', '.join(keywords), summary.text\n",
    "    return isbn, \"\", ''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used multithreading to work at several books in a batch at once. There were several oversights that were looked into when applying the function using something similar to the following function which instead of retrying, raised the Error with the details of the book and error. The oversigts and bugs were then fixed. Later this function, was only getting error when one of the providers were giving us timeout. So, we made this recursive after waiting for 5 seconds for the timeout to go away and continue from where it left off. Also this concatinates the new results with the old ones that we found in previous iterations.\n",
    "\n",
    "<div style=\"background-color: #e7f3ff; padding: 15px; border-radius: 5px; border-left: 5px solid #007acc; color: #444; width: 95%\">\n",
    "    <b>Note:</b> \n",
    "    <br>\n",
    "    We were supervising this function till 1600 books. All that time the function was working perfectly while giving the keywords and summary. After 8000+ books were scraped, the function ran into an error loop (due to the try except clause). When we dug deeper into what happened, we found the last bug which was openLibrary sending us summary as <code>dict</code> sometimes and not <code>str</code>. We fixed that instantly, but after that when we checked the output file, we found out that the summary of most of the books (~97%) were overwritten for some reason. After which we didn't have enough time to scrap that data again and submit this project before deadline and hence we asked another team for their scraped data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './Summaries.xlsx'\n",
    "def scrap_and_save(start_idx):\n",
    "    try:\n",
    "        for i in range(start_idx, 3500):\n",
    "            books = []\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(summary_finder, isbn, header)\n",
    "                    for isbn in data_table['ISBN'][10*i:10*(i+1)]\n",
    "                ]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    isbn, keywords, summary = future.result()\n",
    "                    books.append({\n",
    "                        'isbn': isbn,\n",
    "                        'keywords': keywords,\n",
    "                        'summary': summary\n",
    "                    })\n",
    "                \n",
    "            pd.concat([pd.read_excel(output_file), pd.DataFrame(books)], ignore_index=True).to_excel('./Summaries.xlsx', index=False)\n",
    "    except:\n",
    "        time.sleep(5)\n",
    "        scrap_and_save(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra\n",
    "\n",
    "When trying to clean the data given by the RC, we found out that the RC has even more data that is not only helpful but also crucial in their website. They already had a lot of summary, subjects, description, contributor names which were not in the data provided. And hence we did try to scrap from there, but later stopped doing so after the warning from the faculty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Correction from RC website scraping\n",
    "\n",
    "\n",
    "for i in range(334, 2000):\n",
    "    not_Found = []\n",
    "    cleaned_data_table = pd.DataFrame(columns=[\n",
    "                        'Acc. No.', \n",
    "                        'Acc. Date', \n",
    "                        'ISBN', \n",
    "                        'Title', \n",
    "                        'Edition', \n",
    "                        'Author', \n",
    "                        'Contributor(s)',\n",
    "                        'Series',\n",
    "                        'Publisher', \n",
    "                        'Publication Date', \n",
    "                        'Publisher Location', \n",
    "                        'Description', \n",
    "                        'Subjects', \n",
    "                        'DDC', \n",
    "                        'Summary'\n",
    "                    ])\n",
    "    try:\n",
    "        for acc_no, acc_date, isbn in data_table[10*i:10*(i+1)].itertuples(index=False):\n",
    "            while len(isbn) < 10:\n",
    "                isbn = f'0{isbn}'\n",
    "            # print(isbn)\n",
    "            search_base_link = f'https://opac.daiict.ac.in/cgi-bin/koha/opac-search.pl?q={isbn}'\n",
    "            print('searched', isbn)\n",
    "            search_req = request('GET', search_base_link, headers=header)\n",
    "            rc_soup = BeautifulSoup(search_req.text, 'html.parser')\n",
    "            if rc_soup.find('div', {'id': 'didyoumean'}):\n",
    "                if rc_soup.find('h1', {'id': 'numresults'}).text == 'No results found!':\n",
    "                    not_Found.append((acc_no, acc_date, isbn))\n",
    "                    print(isbn, 'not found')\n",
    "                    print(f'{\"-\"*30}')\n",
    "                    continue\n",
    "                print('clicking results')\n",
    "                results = rc_soup.find_all('a', {'class': 'title'})\n",
    "                for idx, result in enumerate(results):\n",
    "                    search_base_link = f\"https://opac.daiict.ac.in{result.get_attribute_list('href')[0]}\"\n",
    "                    search_req = request('GET', search_base_link, headers=header)\n",
    "                    rc_soup = BeautifulSoup(search_req.text, 'html.parser')\n",
    "                    if isbn == rc_soup.find('span', {'property': 'isbn'}).text.strip():\n",
    "                        break\n",
    "                    print(f'going to next result {idx}/{len(results)}')\n",
    "            title = rc_soup.find('h1', {'class': 'title'}).text\n",
    "            # print(repr(title))\n",
    "            # if rc_soup.find('span', {'property': 'bookEdition'}):\n",
    "            edition = rc_soup.find('span', {'property': 'bookEdition'}).text if rc_soup.find('span', {'property': 'bookEdition'}) else \"\"\n",
    "            # print(repr(edition))\n",
    "            # else:\n",
    "                # edition = \"\"\n",
    "            author = rc_soup.find('span', {'property': 'author'}).text\n",
    "            # print(repr(author))\n",
    "            contributors = [i.text for i in rc_soup.find_all('span', {'property': 'contributor'})] if rc_soup.find('span', {'property': 'contributor'}) else []\n",
    "            # print(repr(contributors))\n",
    "            series = rc_soup.find('span', {'class': 'series'}).a.text if rc_soup.find('span', {'class': 'series'}) else \"\"\n",
    "            # print(repr(series))\n",
    "            publisher = rc_soup.find('span', {'class': 'publisher_name'}).text[:-2]\n",
    "            # print(repr(publisher))\n",
    "            pub_date = rc_soup.find('span', {'class': 'publisher_date'}).text\n",
    "            # print(repr(pub_date))\n",
    "            pub_place = rc_soup.find('span', {'class': 'publisher_place'}).text[:-2]\n",
    "            # print(repr(pub_place))\n",
    "            desc = rc_soup.find('span', {'property': 'description'}).text\n",
    "            # print(repr(desc))\n",
    "            sub = rc_soup.find('span', {'class': 'subjects'}).ul.text.strip().split('\\n') if rc_soup.find('span', {'class': 'subjects'}) else []\n",
    "            # print(repr(sub))\n",
    "            ddc = rc_soup.find('span', {'class': 'ddc'}).ul.text\n",
    "            # print(repr(ddc))\n",
    "            summary = rc_soup.find('span', {'class': 'summary'}).text[9:] if rc_soup.find('span', {'class': 'summary'}) else \"\"\n",
    "            # print(repr(summary))\n",
    "            \n",
    "            book = {\n",
    "                'Acc. No.': acc_no,\n",
    "                'Acc. Date': acc_date,\n",
    "                'ISBN': isbn,\n",
    "                'Title': title,\n",
    "                'Edition': edition,\n",
    "                'Author': author,\n",
    "                'Contributor(s)': contributors,\n",
    "                'Series': series,\n",
    "                'Publisher': publisher, \n",
    "                'Publication Date': pub_date, \n",
    "                'Publisher Location': pub_place, \n",
    "                'Description': desc, \n",
    "                'Subjects': sub, \n",
    "                'DDC': ddc, \n",
    "                'Summary': summary\n",
    "            }\n",
    "            cleaned_data_table = pd.concat([cleaned_data_table, pd.DataFrame([book])], ignore_index=True)\n",
    "            \n",
    "            print(isbn, 'added')\n",
    "            print(f\"{'-'*30}\")\n",
    "\n",
    "        if not_Found:\n",
    "            with open('notFound.txt', 'a') as nf:\n",
    "                for n in not_Found:\n",
    "                    nf.write(f'{n}\\n')\n",
    "        old_df = pd.read_excel('./CleanedData.xlsx')\n",
    "        pd.concat([old_df, cleaned_data_table], ignore_index=True).to_excel('./CleanedData.xlsx', index=False)\n",
    "    except:\n",
    "        raise LookupError(f\"Failed for {isbn} and i={i}\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
